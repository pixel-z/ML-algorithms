{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0_wToNFHMN3"
      },
      "source": [
        "# **Decision Trees**\n",
        "\n",
        "The Wisconsin Breast Cancer Dataset(WBCD) can be found here(https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data)\n",
        "\n",
        "This dataset describes the characteristics of the cell nuclei of various patients with and without breast cancer. The task is to classify a decision tree to predict if a patient has a benign or a malignant tumour based on these features.\n",
        "\n",
        "Attribute Information:\n",
        "```\n",
        "#  Attribute                     Domain\n",
        "   -- -----------------------------------------\n",
        "   1. Sample code number            id number\n",
        "   2. Clump Thickness               1 - 10\n",
        "   3. Uniformity of Cell Size       1 - 10\n",
        "   4. Uniformity of Cell Shape      1 - 10\n",
        "   5. Marginal Adhesion             1 - 10\n",
        "   6. Single Epithelial Cell Size   1 - 10\n",
        "   7. Bare Nuclei                   1 - 10\n",
        "   8. Bland Chromatin               1 - 10\n",
        "   9. Normal Nucleoli               1 - 10\n",
        "  10. Mitoses                       1 - 10\n",
        "  11. Class:                        (2 for benign, 4 for malignant)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DT_l6iUBi4bI"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score \n",
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn import tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "qYdlWpUVHMOB",
        "outputId": "3a7ad57b-f36e-4ac3-85ba-9f0aa476f8b0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CT</th>\n",
              "      <th>UCSize</th>\n",
              "      <th>UCShape</th>\n",
              "      <th>MA</th>\n",
              "      <th>SECSize</th>\n",
              "      <th>BN</th>\n",
              "      <th>BC</th>\n",
              "      <th>NN</th>\n",
              "      <th>Mitoses</th>\n",
              "      <th>Diagnosis</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>10.0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>694</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>695</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>696</th>\n",
              "      <td>5</td>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>3.0</td>\n",
              "      <td>8</td>\n",
              "      <td>10</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>697</th>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4.0</td>\n",
              "      <td>10</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>698</th>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>5.0</td>\n",
              "      <td>10</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>699 rows × 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     CT  UCSize  UCShape  MA  SECSize    BN  BC  NN  Mitoses  Diagnosis\n",
              "0     5       1        1   1        2   1.0   3   1        1          2\n",
              "1     5       4        4   5        7  10.0   3   2        1          2\n",
              "2     3       1        1   1        2   2.0   3   1        1          2\n",
              "3     6       8        8   1        3   4.0   3   7        1          2\n",
              "4     4       1        1   3        2   1.0   3   1        1          2\n",
              "..   ..     ...      ...  ..      ...   ...  ..  ..      ...        ...\n",
              "694   3       1        1   1        3   2.0   1   1        1          2\n",
              "695   2       1        1   1        2   1.0   1   1        1          2\n",
              "696   5      10       10   3        7   3.0   8  10        2          4\n",
              "697   4       8        6   4        3   4.0  10   6        1          4\n",
              "698   4       8        8   5        4   5.0  10   4        1          4\n",
              "\n",
              "[699 rows x 10 columns]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "headers = [\"ID\",\"CT\",\"UCSize\",\"UCShape\",\"MA\",\"SECSize\",\"BN\",\"BC\",\"NN\",\"Mitoses\",\"Diagnosis\"]\n",
        "data = pd.read_csv('breast-cancer-wisconsin.data', na_values='?',    \n",
        "         header=None, index_col=['ID'], names = headers) \n",
        "data = data.reset_index(drop=True)\n",
        "data = data.fillna(0)\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_gQq5qrHMOG"
      },
      "source": [
        "1. a) Implement a decision tree (you can use decision tree implementation from existing libraries)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6R3GmzBHMOH"
      },
      "outputs": [],
      "source": [
        "# train test split\n",
        "X = data.values[:, :-1] \n",
        "Y = data.values[:, -1]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 12) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZ7N9m_mHMOJ"
      },
      "source": [
        "1. b) Train a decision tree object of the above class on the WBC dataset using misclassification rate, entropy and Gini as the splitting metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPxNJ-Q61TJ3"
      },
      "outputs": [],
      "source": [
        "# gini\n",
        "clf_gini = DecisionTreeClassifier(criterion = \"gini\", random_state = 100,max_depth=5)\n",
        "clf_gini.fit(X_train, y_train)\n",
        "y_gini_pred = clf_gini.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ommh9WHU-cLI"
      },
      "outputs": [],
      "source": [
        "# entropy\n",
        "clf_entropy = DecisionTreeClassifier(criterion = \"entropy\", random_state = 100,max_depth=5)\n",
        "clf_entropy.fit(X_train, y_train)\n",
        "y_entropy_pred = clf_entropy.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXEjInvmHMOK"
      },
      "source": [
        "1. c) Report the accuracies in each of the above splitting metrics and give the best result. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49QZvmgNHMOL",
        "outputId": "a61f524c-8f1d-430d-b100-575f38f4e2eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracies:\n",
            "Gini: 95.0 %\n",
            "Entropy: 96.42857142857143 %\n"
          ]
        }
      ],
      "source": [
        "print(\"Accuracies:\")\n",
        "print('Gini:',accuracy_score(y_test,y_gini_pred)*100,'%')\n",
        "print('Entropy:',accuracy_score(y_test,y_entropy_pred)*100,'%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bz_7nYxPHMON"
      },
      "source": [
        "1. d) Experiment with different approaches to decide when to terminate the tree (number of layers, purity measure, etc). Report and give explanations for all approaches. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NX4oHdIfFMul"
      },
      "source": [
        "- We used RandomizedCV and GridSearchCV (from scikit-learn) for model selection, we get the best possible model for decision trees. Most of the models are giving nearly 95%-96% accuracy. \n",
        "- Manual model selection was also done, it gave similar results. But these approaches seemed exhaustive and better for hyper-parameter tuning. For max_depth we tried to keep it low so as to avoid overfitting.\n",
        "\n",
        "- GridSearchCV\n",
        "  - A very common approach for hyperparameter tuning in scikit-learn is GridSearchCV. GridSearchCV is an exhaustive approach, because it checks every single possible combination of hyperparameter values to determine the optimal combination.\n",
        "- RandomizedSearchCV\n",
        "  - This method uses clever shortcut — rather than trying every single unique combination of hyperparameter values, a random search samples a randomly-selected subset of n combinations. The randomized search process requires considerably less compute time and often delivers a similar result. The logic is that by checking enough randomly-chosen combinations on the grid, the search is likely to identify one that is similar to that of an exhaustive process would have identified."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLRI0niJHMOP"
      },
      "outputs": [],
      "source": [
        "# hyper-parameter tuning\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "hyper_params = {\n",
        "    'criterion': ['gini','entropy'],\n",
        "    'max_depth': range(3,20),\n",
        "    'min_samples_leaf': range(5,400),\n",
        "    'random_state': range(0,100)\n",
        "}\n",
        "clf = RandomizedSearchCV(DecisionTreeClassifier(), hyper_params, cv=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X22ODLF7I3EM",
        "outputId": "99680de2-78e8-4904-fa51-211768690e59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'random_state': 97, 'min_samples_leaf': 6, 'max_depth': 4, 'criterion': 'gini'} Score: 0.9409427284427284\n"
          ]
        }
      ],
      "source": [
        "clf.fit(X_train,y_train)\n",
        "print(clf.best_params_,'Score:',clf.best_score_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYNMDDZpJKqz",
        "outputId": "7014a126-f2d0-45c1-933e-62ab8adce882"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entropy: 96.42857142857143 %\n"
          ]
        }
      ],
      "source": [
        "# best parameter using RandomizedSearchCV\n",
        "clf_entropy = DecisionTreeClassifier(criterion = \"entropy\", random_state = 59, max_depth=5)\n",
        "clf_entropy.fit(X_train, y_train)\n",
        "y_entropy_pred = clf_entropy.predict(X_test)\n",
        "print('Entropy:',accuracy_score(y_test,y_entropy_pred)*100,'%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJGboN-4Nufv",
        "outputId": "b9dbb14a-7a46-4ad4-81b7-6a26727af1d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'criterion': 'entropy', 'max_depth': 3, 'min_samples_leaf': 15, 'random_state': 0} Score: 0.9535231660231659\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "hyper_params = {\n",
        "    'criterion': ['gini','entropy'],\n",
        "    'max_depth': range(3,20),\n",
        "    'min_samples_leaf': range(5,100,10),\n",
        "    'random_state': range(0,100,25)\n",
        "}\n",
        "clf = GridSearchCV(DecisionTreeClassifier(), hyper_params, cv=5)\n",
        "clf.fit(X_train,y_train)\n",
        "print(clf.best_params_,'Score:',clf.best_score_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3TJ0CjQOBn9",
        "outputId": "2431e0dc-5bb3-4844-a537-5e89b89c0d1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entropy: 95.0 %\n"
          ]
        }
      ],
      "source": [
        "# best parameter using GridSearchCV\n",
        "clf_entropy = DecisionTreeClassifier(criterion = \"entropy\", random_state = 0, max_depth=3)\n",
        "clf_entropy.fit(X_train, y_train)\n",
        "y_entropy_pred = clf_entropy.predict(X_test)\n",
        "print('Entropy:',accuracy_score(y_test,y_entropy_pred)*100,'%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWAN_wWXHMOQ"
      },
      "source": [
        "2. What is boosting, bagging and  stacking?\n",
        "Which class does random forests belong to and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnO5uqHlHMOR"
      },
      "source": [
        "Answer:    \n",
        "Boosting, bagging and  stacking are very popular ensemble techniques.\n",
        "- **Boosting**:\n",
        "  (Weighted voting)    \n",
        "The idea of boosting is to train weak learners sequentially, each trying to correct its predecessor. The weak learners are sequentially corrected by their predecessors and, in the process, they are converted into strong learners. While combining , boosting pays higher focus on examples which are miss-classified or have higher errors by preceding weak rules.\n",
        "\n",
        "- **Bagging**:   \n",
        "  Objective to reduce variance, not bias of a decision tree. It creates a few subsets of data from the training sample, which is chosen randomly with replacement. Now each collection of subset data is used to prepare their decision trees thus, we end up with an ensemble of various models. The average of all the assumptions from numerous tress is used, which is more powerful than a single decision tree.\n",
        "\n",
        "- **Stacking**:   \n",
        "  Stacking learns from some weak models but as opposed to above two methods the weak learners here are heterogenous (i.e, different learning algorithms are used) and also as opposed to deterministic combining strategy used in above two methods, it learns to combine the base models.\n",
        "\n",
        "- Random forest belongs to bagging class because random forest is an averaging method that aims to reduce the variance of individual trees by randomly selecting many trees from the dataset, and averaging them. So we see that all the weak learners here are decision trees that are trained on a subset of trainng data and while combining mode of the observation is taken, therefore all the decisoin trees are uncorelated. Therefore it belongs to bagging approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pihvGbqLHMOS"
      },
      "source": [
        "3. Implement random forest algorithm using different decision trees . "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXdPP2aIHMOT"
      },
      "outputs": [],
      "source": [
        "# default values are chosen from documentation: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
        "# min_size = min_samples_split as in docs\n",
        "# n_features = subset of features in a decision tree\n",
        "class RandomForest:\n",
        "  def __init__(self, depth=None, min_size=2, n_trees=100, criterion='gini'):\n",
        "    self.X_train, self.y_train, self.depth, self.min_size, self.n_trees, self.criterion = X_train, y_train, depth, min_size, n_trees, criterion\n",
        "    self.dTrees = [self.createTree() for i in range(n_trees)]\n",
        "  \n",
        "  def createTree(self):\n",
        "    return DecisionTreeClassifier(max_depth=self.depth, min_samples_split=self.min_size, criterion=self.criterion)\n",
        "\n",
        "  # n_features is in fit because we should be able to change it according to the data (X_train)\n",
        "  def fit(self, X_train, y_train, n_features=None):\n",
        "    np.random.seed(10)\n",
        "    if n_features == None:\n",
        "      self.n_features = int(np.sqrt(X_train.shape[1]))\n",
        "    else:\n",
        "      self.n_features = n_features\n",
        "\n",
        "    self.indexes = []\n",
        "    for tree_clf in self.dTrees:\n",
        "      # idx = np.random.randint(0,X_train.shape[1]-1,self.n_features)\n",
        "      idx = np.random.permutation(X_train.shape[1])[:self.n_features]\n",
        "      self.indexes.append(idx)\n",
        "\n",
        "      # print(self.indexes)\n",
        "      X_new = []\n",
        "      for i in idx:\n",
        "        X_new.append(X_train[:,i])\n",
        "      X_new = np.array(X_new).T\n",
        "      y_new = np.array(y_train)\n",
        "      \n",
        "      # decision tree fitting (not RandomForest fitting)\n",
        "      tree_clf.fit(X_new,y_new)\n",
        "\n",
        "  def predict(self, X_test):\n",
        "    prediction_list = []\n",
        "    for i in range(self.n_trees):\n",
        "      idx = self.indexes[i]\n",
        "      X_test_new = []\n",
        "      for i in idx:\n",
        "        X_test_new.append(X_test[:,i])\n",
        "      X_test_new = np.array(X_test_new).T \n",
        "      # print(X_test_new.shape)\n",
        "      y_pred = self.dTrees[i].predict(X_test_new)\n",
        "      # y_pred = np.array(y_pred)\n",
        "      # print(y_pred.shape)\n",
        "\n",
        "      prediction_list.append(y_pred)\n",
        "    # prediction_list = np.array(prediction_list)\n",
        "    # print(prediction_list.shape)  # (100=n_trees,140=rows)\n",
        "    prediction_list = np.array(prediction_list).T\n",
        "    prediction_list = prediction_list.astype(int)\n",
        "\n",
        "    voted_y_pred = []\n",
        "    for row in prediction_list:\n",
        "      voted_y_pred.append(np.argmax(np.bincount(row)))\n",
        "    # print(voted_y_pred)\n",
        "    return voted_y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJOn5nNZHMOU"
      },
      "source": [
        "4. Report the accuracies obtained after using the Random forest algorithm and compare it with the best accuracies obtained with the decision trees. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WuPJtspjfvn1"
      },
      "outputs": [],
      "source": [
        "model = RandomForest(depth=3,n_trees=10,criterion='entropy')\n",
        "model.fit(X_train,y_train, 4)\n",
        "y_pred = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWKWCOaj2yWQ",
        "outputId": "c4faed23-f727-4b6e-a4a3-4105f5174d8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 97.85714285714285 %\n"
          ]
        }
      ],
      "source": [
        "print('Accuracy:',accuracy_score(y_test,y_pred)*100,'%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gv11VxqLWvq6"
      },
      "source": [
        "The best accuracies of the decision trees reached till 96%, but random forest performs significantly better at almost 98% accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yj-vNvsYHMOX"
      },
      "source": [
        "5. Submit your solution as a separate pdf in the final zip file of your submission\n",
        "\n",
        "\n",
        "Compute a decision tree with the goal to predict the food review based on its smell, taste and portion size.\n",
        "\n",
        "(a) Compute the entropy of each rule in the first stage.\n",
        "\n",
        "(b) Show the final decision tree. Clearly draw it.\n",
        "\n",
        "Submit a handwritten response. Clearly show all the steps.\n",
        "\n",
        ">**Refer dtree.pdf**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "DecisionTreesAndRandomForests.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "1f8d80d535cfd832283e4e3a1095d2ce45fe6627336684f2622a1965babb2f1c"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 64-bit (windows store)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
